{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "guo1rdTL3fzY"
      },
      "outputs": [],
      "source": [
        "!pip install streamlit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gL6DVqaIOb4l"
      },
      "outputs": [],
      "source": [
        "!pip install python-terrier\n",
        "!pip install nltk\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tBG5L6qm4byv"
      },
      "outputs": [],
      "source": [
        "%%writefile main.py\n",
        "import streamlit as st\n",
        "import pyterrier as pt\n",
        "if not pt.started():\n",
        "  pt.init()\n",
        "\n",
        "\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import PorterStemmer\n",
        "import re\n",
        "pd.set_option('display.max_colwidth', 150)\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "nltk.download('stopwords')\n",
        "stop_words = set(stopwords.words('english'))\n",
        "nltk.download('punkt')\n",
        "from nltk.stem import *\n",
        "from nltk.stem.porter import *\n",
        "import math\n",
        "\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "def scrape_documents(topic, max_documents):\n",
        "    search_url = f\"https://en.wikipedia.org/wiki/{topic}\"\n",
        "\n",
        "    response = requests.get(search_url)\n",
        "\n",
        "    if response.status_code == 200:\n",
        "        soup = BeautifulSoup(response.content, 'html.parser')\n",
        "        document_links = soup.find_all('a', class_='mw-redirect')\n",
        "\n",
        "        results = []\n",
        "\n",
        "        for idx, link in enumerate(document_links, start=1):\n",
        "            title = link.text.strip()\n",
        "            href = 'https://en.wikipedia.org' + link['href']\n",
        "\n",
        "            document_response = requests.get(href)\n",
        "            if document_response.status_code == 200:\n",
        "                document_soup = BeautifulSoup(document_response.content, 'html.parser')\n",
        "                document_content = document_soup.find('div', class_='mw-parser-output')\n",
        "                if document_content:\n",
        "                    text = document_content.get_text(separator='\\n')\n",
        "                else:\n",
        "                    text = \"No content found\"\n",
        "\n",
        "                document_info = {\n",
        "                    'title': title,\n",
        "                    'link': href,\n",
        "                    'text': text\n",
        "                }\n",
        "                results.append(document_info)\n",
        "\n",
        "            if idx == max_documents:\n",
        "                break\n",
        "\n",
        "        return results\n",
        "    else:\n",
        "        print(\"Failed to retrieve page:\", response.status_code)\n",
        "        return []\n",
        "\n",
        "\n",
        "def clean_words_and_remove_spaces(text):\n",
        "    valid_characters = \"abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ\"\n",
        "\n",
        "    cleaned_words = []\n",
        "    for letter in text:\n",
        "        if letter in valid_characters:\n",
        "            cleaned_words.append(letter)\n",
        "        else:\n",
        "            cleaned_words.append(' ')\n",
        "\n",
        "    cleaned_text = ''.join(cleaned_words)\n",
        "    cleaned_text = ' '.join(cleaned_text.split())\n",
        "\n",
        "    return cleaned_text\n",
        "\n",
        "\n",
        "def put_in_data_frame(all_results):\n",
        "  df = pd.DataFrame(all_results)\n",
        "  df['docno'] = range(1, len(df) + 1)\n",
        "  return df\n",
        "\n",
        "\n",
        "def Preprocess_my_text(text):\n",
        "    stemmer = PorterStemmer()\n",
        "    text = re.sub(r\"http\\S+\", \" \", text)\n",
        "    text = re.sub(r\"RT \", \" \", text)\n",
        "    text = re.sub(r\"@[\\w]*\", \" \", text)\n",
        "    text = re.sub(r\"[\\.\\,\\#_\\|\\:\\?\\?\\/\\=]\", \" \", text)\n",
        "    text = re.sub(r'\\t', ' ', text)\n",
        "    text = re.sub(r'\\n', ' ', text)\n",
        "    text = re.sub(r\"\\s+\", \" \", text)\n",
        "    tokens = word_tokenize(text)\n",
        "    filtered_stemmed_tokens = [stemmer.stem(word.lower()) for word in tokens if word.lower() not in stop_words]\n",
        "    return ' '.join(filtered_stemmed_tokens)\n",
        "\n",
        "\n",
        "def find_expand_and_rank(df,query):\n",
        "  # indexing\n",
        "  indexer = pt.DFIndexer(\"./DatasetIndex\", overwrite=True)\n",
        "  index_ref = indexer.index(df[\"cleaned_text\"].astype(str), df[\"docno\"].astype(str))\n",
        "  index_ref.toString()\n",
        "  index = pt.IndexFactory.of(index_ref)\n",
        "\n",
        "  #expand\n",
        "  print(query)\n",
        "  QE = pt.BatchRetrieve(index, wmodel=\"TF_IDF\", controls={\"qemodel\" : \"Bo1\", \"qe\" : \"on\"}).search(' '.join(query))\n",
        "\n",
        "  return QE\n",
        "\n",
        "def return_the_links(df, q_results):\n",
        "    df[\"docno\"] = df[\"docno\"].astype(str)\n",
        "    q_results[\"docno\"] = q_results[\"docno\"].astype(str)\n",
        "\n",
        "    merged_df = pd.merge(df, q_results, on=\"docno\", how=\"inner\")\n",
        "    links_list = [(title, link) for title, link in zip(merged_df[\"title\"], merged_df[\"link\"])]\n",
        "    return links_list\n",
        "\n",
        "\n",
        "def search(query,topic):\n",
        "\n",
        "    if topic.lower() == \"auto\":\n",
        "      topic = \"science\"\n",
        "\n",
        "    documents_per_topic = 20 #changable\n",
        "    all_results = scrape_documents(topic, documents_per_topic)\n",
        "    # for result in all_results:\n",
        "    #     print(\"Title:\", result['title'])\n",
        "\n",
        "    for i in range(len(all_results)):\n",
        "      document = all_results[i]\n",
        "      document['text'] = clean_words_and_remove_spaces(document['text'])\n",
        "\n",
        "    df = put_in_data_frame(all_results)\n",
        "    df['cleaned_text'] = df['text'].apply(Preprocess_my_text).apply(clean_words_and_remove_spaces)\n",
        "    df.drop(columns = {\"text\"},inplace = True)\n",
        "    df.dropna(inplace = True)\n",
        "\n",
        "    query = Preprocess_my_text(query).split()\n",
        "    q_results = find_expand_and_rank(df,query)\n",
        "\n",
        "    links_list = return_the_links(df, q_results)\n",
        "\n",
        "    return links_list\n",
        "\n",
        "def main():\n",
        "    st.title(\"Browser\")\n",
        "    st.text(\"Enter the general topic then what you are searching for :)\")\n",
        "\n",
        "    search_topic = st.text_input(\"Topic:\")\n",
        "    search_query = st.text_input(label=\"Query:\")\n",
        "    submit = st.button(\"Search\")\n",
        "\n",
        "\n",
        "    # submit = st.form_submit_button(\"Abracadabra!\", use_container_width=True)\n",
        "\n",
        "    if submit and search_query and search_topic:\n",
        "        results = search(search_query,search_topic)\n",
        "        st.write(\"Search Results:\")\n",
        "        for title, link in results:\n",
        "            st.write(f\"{title}: {link}\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QyhSan19Kvl6",
        "outputId": "c6cc1181-810e-4190-d626-c50d1b6c92d6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Collecting usage statistics. To deactivate, set browser.gatherUsageStats to false.\n",
            "\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[34m\u001b[1m  You can now view your Streamlit app in your browser.\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[34m  Local URL: \u001b[0m\u001b[1mhttp://localhost:8501\u001b[0m\n",
            "\u001b[34m  Network URL: \u001b[0m\u001b[1mhttp://172.28.0.12:8501\u001b[0m\n",
            "\u001b[34m  External URL: \u001b[0m\u001b[1mhttp://35.197.39.220:8501\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[1G\u001b[0JNeed to install the following packages:\n",
            "  localtunnel@2.0.2\n",
            "Ok to proceed? (y) \u001b[20G"
          ]
        }
      ],
      "source": [
        "!streamlit run main.py & npx localtunnel --port 8501"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}